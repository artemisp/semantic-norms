{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "sys.path.insert(1, '../../')\n",
    "import eval\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "import math\n",
    "def s(x):\n",
    "    return 1 / (1 + math.e**(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"feature_norms\" # \"concept_properties\", \"feature_norms\", \"memory_colors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun2prop = pickle.load(open(f\"../data/datasets/{DATASET}/noun2property/noun2prop.p\", \"rb\"))\n",
    "gpt3_scores = pickle.load(open(f\"../data/datasets/{DATASET}/GPT3/gpt3_predicts.txt\", \"rb\"))\n",
    "roberta_scores = pickle.load(open(f\"../output/output_{DATASET}/roberta-large+singular_generally.p\", \"rb\"))\n",
    "bert_scores = pickle.load(open(f\"../output/output_{DATASET}/bert-large-uncased+plural_most.p\", \"rb\"))\n",
    "vilt_scores = pickle.load(open(f\"../output/output_{DATASET}/vilt+plural+10.p\", \"rb\"))\n",
    "clip_scores = pickle.load(open(f\"../data/datasets/{DATASET}/CLIP/clip_scores.p\", \"rb\"))\n",
    "combined_scores = pickle.load(open(f\"../data/datasets/{DATASET}/CEM/combine_scores.p\", \"rb\"))\n",
    "ngram_scores = pickle.load(open(\"../data/datasets/feature_norms/ngram_scores.p\", \"rb\"))\n",
    "gpt_scores = pickle.load(open(f\"../output/output_{DATASET}/gpt2-large+plural_most.p\", \"rb\"))\n",
    "\n",
    "\n",
    "candidate_adjs = []\n",
    "for noun, props in noun2prop.items():\n",
    "    candidate_adjs += props\n",
    "candidate_adjs = list(set(candidate_adjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concreteness = {w: c / 5 for w, c in pickle.load(open(\"../data/word2concreteness.M.p\", \"rb\")).items()}\n",
    "all_words = list(concreteness.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:15<00:00, 13.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "prop2concretness = {}\n",
    "for prop in tqdm(candidate_adjs):\n",
    "    if prop in concreteness:\n",
    "        prop2concretness[prop] = concreteness[prop]\n",
    "    else:\n",
    "        sims = []\n",
    "        for word in all_words:\n",
    "            sims.append((word, similar(word, prop)))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        prop2concretness[prop] = concreteness[sims[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noun2concretness_pred = pickle.load(open(\"../data/MRD_noun2concreteness.p\", \"rb\"))\n",
    "prop2concretness_pred = pickle.load(open(\"/nlp/data/yueyang/prototypicality/predicted_adjective2concreteness.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [00:10<00:00, 47.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "noun2concretness = {}\n",
    "for noun in tqdm(noun2prop):\n",
    "    if noun in concreteness:\n",
    "        noun2concretness[noun] = concreteness[noun]\n",
    "    else:\n",
    "        sims = []\n",
    "        for word in all_words:\n",
    "            sims.append((word, similar(word, noun)))\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        noun2concretness[noun] = concreteness[sims[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1 acc:  0.3988212180746562\n",
      "top5 acc:  0.7583497053045186\n",
      "recall@5:  0.3997357096080082\n",
      "recall@10:  0.5253937069261234\n",
      "MRR: 0.25094019585865684\n",
      "Median rank: 11.0\n",
      "Mean rank: 24.71105527638191\n",
      "\n",
      "39.9 & 75.8 & 40.0 & 52.5 & 0.251\n"
     ]
    }
   ],
   "source": [
    "noun2predicts = {}\n",
    "lambs = []\n",
    "for noun, c_scores in clip_scores.items():\n",
    "    b_order = {p:i for i, p in enumerate(roberta_scores[noun])}\n",
    "    c_order = {p:i for i, p in enumerate(c_scores)}\n",
    "    combine_order = {}\n",
    "    for prop, rank in c_order.items():\n",
    "        # lamb = random.uniform(0, 1)\n",
    "        lamb = prop2concretness_pred[prop] / 5\n",
    "        # lambs.append(lamb)\n",
    "        # lamb = 0.5\n",
    "        combine_order[prop] = (1-lamb) * b_order[prop] + lamb * rank\n",
    "        # combine_order[prop] = min(b_order[prop], rank)\n",
    "    predicts = [(p, r) for p, r in combine_order.items()]\n",
    "    predicts.sort(key=lambda x: x[0])\n",
    "    predicts.sort(key=lambda x: x[1], reverse=False)\n",
    "    noun2predicts[noun] = [pred[0] for pred in predicts]\n",
    "    \n",
    "acc_1 = eval.evaluate_acc(noun2predicts, noun2prop, 1, True)\n",
    "acc_5 = eval.evaluate_acc(noun2predicts, noun2prop, 5, True)\n",
    "r_5 = eval.evaluate_recall(noun2predicts, noun2prop, 5, True)\n",
    "r_10 = eval.evaluate_recall(noun2predicts, noun2prop, 10, True)\n",
    "mrr = eval.evaluate_rank(noun2predicts, noun2prop, True)[1]\n",
    "print(\" & \".join([str(round(100*acc_1,1)), str(round(100*acc_5,1)), str(round(100*r_5,1)), str(round(100*r_10,1)), str(round(mrr, 3))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(noun2predicts, open(\"combine_scores.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "mrrs = []\n",
    "recalls = []\n",
    "for k in range(11):\n",
    "    lamb = k * 0.1\n",
    "    noun2predicts = {}\n",
    "    for noun, c_scores in vilt_scores.items():\n",
    "        b_order = {p:i for i, p in enumerate(roberta_scores[noun])}\n",
    "        c_order = {p:i for i, p in enumerate(c_scores)}\n",
    "        combine_order = {}\n",
    "        for prop, rank in c_order.items():\n",
    "            combine_order[prop] = (1-lamb) * b_order[prop] + lamb * rank\n",
    "        predicts = [(p, r) for p, r in combine_order.items()]\n",
    "        predicts.sort(key=lambda x: x[0])\n",
    "        predicts.sort(key=lambda x: x[1], reverse=False)\n",
    "        noun2predicts[noun] = [pred[0] for pred in predicts]\n",
    "    accs.append(round(100 * eval.evaluate_acc(noun2predicts, noun2prop, 1, False), 1))\n",
    "    mrrs.append(eval.evaluate_rank(noun2predicts, noun2prop, False)[1])\n",
    "    recalls.append(eval.evaluate_recall(noun2predicts, noun2prop, 10, False))\n",
    "    # for k in [1, 3, 5]:\n",
    "    #     eval.evaluate_acc(noun2predicts, noun2prop, k, True)\n",
    "    # mrrs.append(eval.evaluate_rank(noun2predicts, noun2prop, False)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.6, 31.0, 34.6, 34.2, 34.2, 34.4, 35.2, 32.8, 32.6, 31.4, 27.9]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d7325128c788ebe58d0531710a6fa19c8b08d83880d6b610174ef1183dc68e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
