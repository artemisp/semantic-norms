{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"../data/datasets/concept_properties/raw_data/norms.dat\", \"r\") as f:\n",
    "    for line in f.readlines()[1:]:\n",
    "        domain, feature_type, concept, feature, freq =  line[:-1].split(\"\\t\")[:5]\n",
    "        if feature[:2] == \"is\" and len(feature.split(\" \")) == 2 and \"_\" not in concept:\n",
    "            if \"_\" in feature.split(\" \")[1]:\n",
    "                features = feature.split(\" \")[1].split(\"_\")\n",
    "                for f in features:\n",
    "                    data.append((domain, feature_type, concept, f, int(freq)))\n",
    "            else:\n",
    "                data.append((domain, feature_type, concept.lower(), feature.split(\" \")[1].lower(), int(freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = list(set([d[2] for d in data]))\n",
    "props = list(set([d[3] for d in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "brysbaert_adjectives = []\n",
    "brysbaert_data = pd.read_csv('/nlp/data/yueyang/prototypicality/clean/concreteness_predictor/Concreteness_ratings_Brysbaert_et_al_BRM.txt', delimiter='\\t')\n",
    "for i,row in brysbaert_data.iterrows():\n",
    "    if row['Dom_Pos'] == 'Adjective':\n",
    "        brysbaert_adjectives.append(row['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 802/802 [00:00<00:00, 2577.47it/s]\n"
     ]
    }
   ],
   "source": [
    "adjs = []\n",
    "non_adjs = []\n",
    "for prop in tqdm(props):\n",
    "    try:\n",
    "        top_pos = nltk.FreqDist(t for w, t in brown.tagged_words() if w.lower() == prop).most_common()[0][0]\n",
    "    except:\n",
    "        top_pos = 'JJ'\n",
    "    if 'JJ' in top_pos:\n",
    "        adjs.append(prop)\n",
    "    else:\n",
    "        non_adjs.append(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7334/7334 [00:00<00:00, 1351182.71it/s]\n"
     ]
    }
   ],
   "source": [
    "noun2props = {noun: [] for noun in nouns}\n",
    "for d in tqdm(data):\n",
    "    if d[2] in noun2props:\n",
    "        if d[3] not in noun2props[d[2]] and d[3] not in non_adjs and d[-1] >= 3:\n",
    "            noun2props[d[2]].append(d[3])\n",
    "del_nouns = []\n",
    "for noun, props in noun2props.items():\n",
    "    if props == []:\n",
    "        del_nouns.append(noun)\n",
    "for noun in del_nouns:\n",
    "    del noun2props[noun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_adjs = []\n",
    "for noun, props in noun2props.items():\n",
    "    candidate_adjs += props\n",
    "candidate_adjs = list(set(candidate_adjs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for noun, props in noun2props.items():\n",
    "    count.append(len(props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.590381426202322"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(noun2props, open(\"../data/datasets/concept_properties/noun2property/noun2prop.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun2det = pickle.load(open(\"inter_data/noun2det_CSLB.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncountable_nouns = []\n",
    "with open(\"inter_data/uncountable_nouns.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        uncountable_nouns.append(line[:-1].lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2templates = {\"plural_most\": [\"most {} are [MASK].\", \"{} is [MASK].\"], \n",
    "                    \"singular_can_be\": [\"a {} can be [MASK].\", \"{} can be [MASK].\"],\n",
    "                    \"singular_usually\": [\"a {} is usually [MASK].\", \"{} is usually [MASK].\"], \n",
    "                    \"singular_generally\": [\"a {} is generally [MASK].\", \"{} is generally [MASK].\"],\n",
    "                    \"singular\": [\"a {} is [MASK].\", \"{} is [MASK].\"],\n",
    "                    \"plural_all\": [\"all {} are [MASK].\", \"{} is [MASK].\"],\n",
    "                    \"plural_can_be\": [\"{} can be [MASK].\", \"{} can be [MASK].\"],\n",
    "                    \"plural_generally\": [\"{} are generally [MASK].\", \"{} is generally [MASK].\"], \n",
    "                    \"plural_some\": [\"some {} are [MASK].\", \"{} is [MASK].\"], \n",
    "                    \"plural_usually\": [\"{} are usually [MASK].\", \"{} is usually [MASK].\"], \n",
    "                    \"plural\": [\"{} are [MASK].\", \"{} is [MASK].\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,urllib,json\n",
    "def countable(noun):\n",
    "    thing = re.sub(' ', '\\+', noun)\n",
    "    url = 'https://books.google.com/ngrams/graph?content=many+' + thing + '%2C+much+' + thing + '&year_start=1980&year_end=2000'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html = response.read()\n",
    "    thing = re.sub('\\+', ' ', thing)\n",
    "    for data in html.decode(\"utf-8\").split(\"\\n\"):\n",
    "        if \"ngrams.data\" in data and len(data) > 200:\n",
    "            break\n",
    "    try:\n",
    "        many_data = json.loads(re.search('\\{\"ngram\": \"many ' + thing + '\".*?\\}', data, re.IGNORECASE).group(0))['timeseries']\n",
    "        many = sum(many_data) / float(len(many_data))\n",
    "    except:\n",
    "        many = 0\n",
    "    try:\n",
    "        much_data = json.loads(re.search('\\{\"ngram\": \"much ' + thing + '\".*?\\}', data, re.IGNORECASE).group(0))['timeseries']\n",
    "        much = sum(much_data) / float(len(much_data))\n",
    "    except:\n",
    "        much = 0\n",
    "    return many > much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "from tqdm import tqdm\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_type, templates in prompt2templates.items():\n",
    "    with open(\"../data/datasets/concept_properties/queries/\" + prompt_type + \".prop\", \"w\") as f:\n",
    "        for noun in nouns:\n",
    "            if lemmatizer.lemmatize(noun) not in uncountable_nouns:\n",
    "                if \"plural\" in prompt_type:\n",
    "                    plural_noun = p.plural(lemmatizer.lemmatize(noun))\n",
    "                    sent = templates[0].format(plural_noun)\n",
    "                else:\n",
    "                    sent = templates[0].format(lemmatizer.lemmatize(noun))\n",
    "                    det = noun2det[noun]\n",
    "                    if det == \"an\":\n",
    "                        sent = sent.replace(\"a \", \"an \")\n",
    "            else:\n",
    "                sent = templates[1].format(lemmatizer.lemmatize(noun))\n",
    "            f.write(noun + \" :: \" + sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "noun2props = pickle.load(open(\"../data/datasets/concept_properties/noun2property/noun2prop.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "all_props = []\n",
    "count_len = []\n",
    "for noun, props in noun2props.items():\n",
    "    count += len(props)\n",
    "    all_props += props\n",
    "    count_len.append(len(props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "noun2prop_CSLB = pickle.load(open(\"../data/datasets/concept_properties/noun2property/noun2prop.p\", \"rb\"))\n",
    "noun2prop_MRD = pickle.load(open(\"../data/datasets/feature_norms/noun2property/noun2prop.p\", \"rb\"))\n",
    "noun2prop_memory_color = pickle.load(open(\"../data/datasets/memory_color/noun2property/noun2prop.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSLB_nouns = list(noun2prop_CSLB.keys())\n",
    "MRD_nouns = list(noun2prop_MRD.keys())\n",
    "memory_color_nouns = list(noun2prop_memory_color.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_nouns = []\n",
    "for noun in CSLB_nouns:\n",
    "    if noun not in MRD_nouns and noun not in memory_color_nouns:\n",
    "        valid_nouns.append(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test_nouns = random.sample(valid_nouns, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun2prop_train = {}\n",
    "for noun in test_nouns:\n",
    "    noun2prop_train[noun] = noun2prop_CSLB[noun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(noun2prop_train, open(\"../data/datasets/concept_properties/noun2property/noun2prop_train.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun2prop_test = {}\n",
    "for noun in CSLB_nouns:\n",
    "    if noun not in test_nouns:\n",
    "        noun2prop_test[noun] = noun2prop_CSLB[noun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(noun2prop_test, open(\"../data/datasets/concept_properties/noun2property/noun2prop_test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun2prop = {}\n",
    "with open(\"../data/datasets/feature_norms/raw_data/McRae_properties.gold\", \"r\") as f:\n",
    "    for raw_data in f.readlines():\n",
    "        noun = raw_data.split(\"\\t\")[0]\n",
    "        prop = raw_data.split(\"\\t\")[1][3:-1]\n",
    "        if noun not in noun2prop:\n",
    "            if '/' in prop:\n",
    "                noun2prop[noun] = prop.split('/')\n",
    "            else:\n",
    "                noun2prop[noun] = [prop]\n",
    "        else:\n",
    "            if '/' in prop:\n",
    "                noun2prop[noun] += prop.split('/')\n",
    "            else:\n",
    "                noun2prop[noun].append(prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "import nltk\n",
    "import copy\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "engine = inflect.engine()\n",
    "Lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/datasets/memory_color/raw_data/memory_color.txt\", \"r\") as f:\n",
    "    data = []\n",
    "    for line in f.readlines():\n",
    "        data.append(line[:-1].split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singulars = []\n",
    "plurals = []\n",
    "item2color = {}\n",
    "for descriptor, item, color in data:\n",
    "    s = Lem.lemmatize(item)\n",
    "    p = engine.plural(s)\n",
    "    singulars.append(s)\n",
    "    plurals.append(p)\n",
    "    item2color[item] = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inter_data/singular_plural.txt\", \"r\") as f:\n",
    "    ps_data = []\n",
    "    for line in f.readlines():\n",
    "        ps_data.append(line[:-1].split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "for i, (descriptor, item, color) in enumerate(data):\n",
    "    final_data.append({\"descriptor\": descriptor, \"item raw\": item, \"item singular\": ps_data[i][0], \"item plural\": ps_data[i][1], \"countable\": ps_data[i][2], \"color\": color})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2templates = {\"plural_most\": [\"most {} are [MASK].\", \"{} is [MASK].\"], \n",
    "                    \"singular_can_be\": [\"a {} can be [MASK].\", \"{} can be [MASK].\"],\n",
    "                    \"singular_usually\": [\"a {} is usually [MASK].\", \"{} is usually [MASK].\"], \n",
    "                    \"singular_generally\": [\"a {} is generally [MASK].\", \"{} is generally [MASK].\"],\n",
    "                    \"singular\": [\"a {} is [MASK].\", \"{} is [MASK].\"],\n",
    "                    \"plural_all\": [\"all {} are [MASK].\", \"{} is [MASK].\"],\n",
    "                    \"plural_can_be\": [\"{} can be [MASK].\", \"{} can be [MASK].\"],\n",
    "                    \"plural_generally\": [\"{} are generally [MASK].\", \"{} is generally [MASK].\"], \n",
    "                    \"plural_some\": [\"some {} are [MASK].\", \"{} is [MASK].\"], \n",
    "                    \"plural_usually\": [\"{} are usually [MASK].\", \"{} is usually [MASK].\"], \n",
    "                    \"plural\": [\"{} are [MASK].\", \"{} is [MASK].\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_type, templates in prompt2templates.items():\n",
    "    with open(\"../data/datasets/memory_color/noun2property/queries/\" + prompt_type + \".prop\", \"w\") as f:\n",
    "        for data in final_data:\n",
    "            if data[\"countable\"] == \"1\":\n",
    "                if \"plural\" in prompt_type:\n",
    "                    if len(data[\"descriptor\"]) < 5:\n",
    "                        sent = templates[0].format(data['item plural'])\n",
    "                    else:\n",
    "                        descriptor = copy.deepcopy(data[\"descriptor\"])\n",
    "                        if descriptor == \"the animal\":\n",
    "                            sent = templates[0].format(descriptor + \" \" + data['item plural'])\n",
    "                        else:\n",
    "                            if descriptor == \"the outside of a\":\n",
    "                                descriptor = \"the outside of\"\n",
    "                            elif descriptor == \"the inside of a\":\n",
    "                                descriptor = \"the inside of\"\n",
    "                            sent = descriptor + \" \" + templates[0].format(data['item plural'])\n",
    "                else:\n",
    "                    if len(data[\"descriptor\"]) <= 1:\n",
    "                        sent = templates[0].format(data['item singular'])\n",
    "                    elif data[\"descriptor\"] == \"an\":\n",
    "                        sent = templates[0].format(data['item singular']).replace(\"a \", \"an \")\n",
    "                    else:\n",
    "                        descriptor = copy.deepcopy(data[\"descriptor\"])\n",
    "                        if descriptor == \"the outside of a\":\n",
    "                            descriptor = \"the outside of\"\n",
    "                        elif descriptor == \"the inside of a\":\n",
    "                            descriptor = \"the inside of\"\n",
    "                        sent = descriptor + \" \" + templates[0].format(data['item singular'])\n",
    "            else:\n",
    "                sent = templates[1].format(data['item raw'])\n",
    "                if data['item raw'] == \"jeans\":\n",
    "                    sent = sent.replace(\" is \", \" are \")\n",
    "            f.write(data['item raw'] + \" :: \" + sent + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun2prop = {}\n",
    "for data in final_data:\n",
    "    noun2prop[data['item raw']] = [data['color']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "item2image_url = {}\n",
    "with open(\"../data/datasets/memory_color/raw_data/memory_colors.jsonl\", \"rb\") as f:\n",
    "    for line in f.readlines():\n",
    "        data = json.loads(line.decode('utf-8'))\n",
    "        item2image_url[data['item']] = data['picture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, url in tqdm(item2image_url.items()):\n",
    "    try:\n",
    "        img_data = requests.get(url).content\n",
    "        with open(\"../data/datasets/memory_color/images/gt_images/\" + item + '.png', 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_image_download import simple_image_download as simp\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "from requests.exceptions import Timeout\n",
    "response = simp.simple_image_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_images = [f.split(\".\")[0] for f in os.listdir(\"../data/datasets/memory_color/images/gt_images/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, url in tqdm(item2image_url.items()):\n",
    "    if item in done_images:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            img_data = requests.get(url).content\n",
    "            with open('png/{}.png'.format(item), 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = requests.get(url).content\n",
    "with open('image_name.png', 'wb') as handler:\n",
    "    handler.write(img_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1452b780fd9fd147b4e79b0b7856474b538f4a3d1bd2a13218246f2c3cd5e36c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
